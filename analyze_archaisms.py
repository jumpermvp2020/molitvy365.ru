#!/usr/bin/env python3
"""
–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Å–ª–æ–≤ –Ω–∞ –ø—Ä–µ–¥–º–µ—Ç –∞—Ä—Ö–∞–∏–∑–º–æ–≤
–ß–∏—Ç–∞–µ—Ç —Ñ–∞–π–ª—ã —Å–æ —Å–ª–æ–≤–∞–º–∏ –∏ –ø–æ–∑–≤–æ–ª—è–µ—Ç –≤—Ä—É—á–Ω—É—é –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å –∏—Ö
"""

import os
import json
from typing import Set, Dict

def load_existing_archaisms() -> Set[str]:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∞—Ä—Ö–∞–∏–∑–º—ã –∏–∑ —Ñ–∞–π–ª–∞"""
    archaisms_file = "extracted_words/archaisms.txt"
    archaisms = set()
    
    if os.path.exists(archaisms_file):
        with open(archaisms_file, 'r', encoding='utf-8') as f:
            for line in f:
                word = line.strip()
                if word and not word.startswith('#'):
                    archaisms.add(word)
    
    return archaisms

def save_archaisms(archaisms: Set[str]):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∞—Ä—Ö–∞–∏–∑–º—ã –≤ —Ñ–∞–π–ª"""
    archaisms_file = "extracted_words/archaisms.txt"
    
    with open(archaisms_file, 'w', encoding='utf-8') as f:
        f.write("# –°–ª–æ–≤–∞—Ä—å –∞—Ä—Ö–∞–∏–∑–º–æ–≤ –∏–∑ –º–æ–ª–∏—Ç–≤\n")
        f.write("# –°–ª–æ–≤–∞, –∫–æ—Ç–æ—Ä—ã–µ —è–≤–ª—è—é—Ç—Å—è –∞—Ä—Ö–∞–∏—á–Ω—ã–º–∏ –∏ –Ω—É–∂–¥–∞—é—Ç—Å—è –≤ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–º –ø–µ—Ä–µ–≤–æ–¥–µ\n\n")
        
        for word in sorted(archaisms):
            f.write(f"{word}\n")

def analyze_word_chunk(chunk_file: str, existing_archaisms: Set[str]) -> Set[str]:
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –æ–¥–∏–Ω —Ñ–∞–π–ª —Å–æ —Å–ª–æ–≤–∞–º–∏"""
    print(f"\nüîç –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ñ–∞–π–ª: {chunk_file}")
    
    with open(chunk_file, 'r', encoding='utf-8') as f:
        lines = f.readlines()
    
    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ—á–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏
    words = []
    for line in lines:
        line = line.strip()
        if line and not line.startswith('#') and not line.startswith('–í—Å–µ–≥–æ'):
            words.append(line)
    
    print(f"üìä –°–ª–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞: {len(words)}")
    
    new_archaisms = set()
    
    for i, word in enumerate(words, 1):
        if word in existing_archaisms:
            continue  # –£–∂–µ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–æ
            
        print(f"\n[{i}/{len(words)}] –°–ª–æ–≤–æ: '{word}'")
        print("–≠—Ç–æ –∞—Ä—Ö–∞–∏–∑–º? (y/n/s - –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å —Ñ–∞–π–ª/q - –≤—ã–π—Ç–∏)")
        
        choice = input().lower().strip()
        
        if choice == 'q':
            return new_archaisms
        elif choice == 's':
            print("–ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ñ–∞–π–ª...")
            return new_archaisms
        elif choice == 'y':
            new_archaisms.add(word)
            print(f"‚úÖ '{word}' –¥–æ–±–∞–≤–ª–µ–Ω –≤ –∞—Ä—Ö–∞–∏–∑–º—ã")
        elif choice == 'n':
            print(f"‚ùå '{word}' –Ω–µ –∞—Ä—Ö–∞–∏–∑–º")
        else:
            print("–ù–µ–≤–µ—Ä–Ω—ã–π –≤—ã–±–æ—Ä, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º...")
    
    return new_archaisms

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("üîç –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å–ª–æ–≤ –Ω–∞ –ø—Ä–µ–¥–º–µ—Ç –∞—Ä—Ö–∞–∏–∑–º–æ–≤")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∞—Ä—Ö–∞–∏–∑–º—ã
    existing_archaisms = load_existing_archaisms()
    print(f"üìö –£–∂–µ –Ω–∞–π–¥–µ–Ω–æ –∞—Ä—Ö–∞–∏–∑–º–æ–≤: {len(existing_archaisms)}")
    
    if existing_archaisms:
        print("–°—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∞—Ä—Ö–∞–∏–∑–º—ã:")
        for arch in sorted(list(existing_archaisms)[:10]):
            print(f"  - {arch}")
        if len(existing_archaisms) > 10:
            print(f"  ... –∏ –µ—â–µ {len(existing_archaisms) - 10}")
    
    # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ —Ñ–∞–π–ª—ã —Å–æ —Å–ª–æ–≤–∞–º–∏
    words_dir = "extracted_words"
    chunk_files = []
    
    for filename in os.listdir(words_dir):
        if filename.startswith("words_chunk_") and filename.endswith(".txt"):
            chunk_files.append(os.path.join(words_dir, filename))
    
    chunk_files.sort()
    
    print(f"\nüìÅ –ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ —Å–æ —Å–ª–æ–≤–∞–º–∏: {len(chunk_files)}")
    
    # –°–ø—Ä–∞—à–∏–≤–∞–µ–º, —Å –∫–∞–∫–æ–≥–æ —Ñ–∞–π–ª–∞ –Ω–∞—á–∞—Ç—å
    if existing_archaisms:
        print("\n–° –∫–∞–∫–æ–≥–æ —Ñ–∞–π–ª–∞ –Ω–∞—á–∞—Ç—å –∞–Ω–∞–ª–∏–∑? (–Ω–æ–º–µ—Ä —Ñ–∞–π–ª–∞ –∏–ª–∏ 'all' –¥–ª—è –≤—Å–µ—Ö)")
        start_choice = input().strip()
        
        if start_choice.isdigit():
            start_index = int(start_choice) - 1
            if 0 <= start_index < len(chunk_files):
                chunk_files = chunk_files[start_index:]
            else:
                print("–ù–µ–≤–µ—Ä–Ω—ã–π –Ω–æ–º–µ—Ä —Ñ–∞–π–ª–∞")
                return
        elif start_choice.lower() != 'all':
            print("–ù–∞—á–∏–Ω–∞–µ–º —Å –ø–µ—Ä–≤–æ–≥–æ —Ñ–∞–π–ª–∞")
    
    all_archaisms = existing_archaisms.copy()
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π —Ñ–∞–π–ª
    for i, chunk_file in enumerate(chunk_files):
        print(f"\n{'='*50}")
        print(f"–§–∞–π–ª {i+1}/{len(chunk_files)}: {os.path.basename(chunk_file)}")
        
        new_archaisms = analyze_word_chunk(chunk_file, all_archaisms)
        all_archaisms.update(new_archaisms)
        
        print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
        print(f"  –ù–æ–≤—ã—Ö –∞—Ä—Ö–∞–∏–∑–º–æ–≤ –≤ —Ñ–∞–π–ª–µ: {len(new_archaisms)}")
        print(f"  –í—Å–µ–≥–æ –∞—Ä—Ö–∞–∏–∑–º–æ–≤: {len(all_archaisms)}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        save_archaisms(all_archaisms)
        
        # –°–ø—Ä–∞—à–∏–≤–∞–µ–º, –ø—Ä–æ–¥–æ–ª–∂–∞—Ç—å –ª–∏
        if i < len(chunk_files) - 1:
            print(f"\n–ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å –∞–Ω–∞–ª–∏–∑ —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ñ–∞–π–ª–∞? (y/n)")
            continue_choice = input().lower().strip()
            if continue_choice != 'y':
                break
    
    print(f"\n‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω!")
    print(f"üìä –ò—Ç–æ–≥–æ –Ω–∞–π–¥–µ–Ω–æ –∞—Ä—Ö–∞–∏–∑–º–æ–≤: {len(all_archaisms)}")
    print(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: extracted_words/archaisms.txt")

if __name__ == "__main__":
    main()
