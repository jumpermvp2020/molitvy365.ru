#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –≤—Å–µ—Ö —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤ –∏–∑ –º–æ–ª–∏—Ç–≤
–†–∞–∑–±–∏–≤–∞–µ—Ç —Å–ª–æ–≤–∞ –Ω–∞ —Ñ–∞–π–ª—ã –ø–æ 500 —Å–ª–æ–≤ –¥–ª—è —É–¥–æ–±–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
"""

import json
import os
import re
from collections import Counter
from typing import Set, List

def extract_words_from_text(text: str) -> Set[str]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –≤—Å–µ —Å–ª–æ–≤–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
    # –£–±–∏—Ä–∞–µ–º –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è –∏ —Ä–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Å–ª–æ–≤–∞
    words = re.findall(r'\b[–∞-—è—ë]+\b', text.lower())
    return set(words)

def process_all_prayers() -> Set[str]:
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤—Å–µ —Ñ–∞–π–ª—ã –º–æ–ª–∏—Ç–≤ –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞"""
    prayers_dir = "data/prayers"
    all_words = set()
    
    if not os.path.exists(prayers_dir):
        print(f"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è {prayers_dir} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
        return all_words
    
    files = [f for f in os.listdir(prayers_dir) if f.endswith('.json')]
    print(f"–ù–∞–π–¥–µ–Ω–æ {len(files)} —Ñ–∞–π–ª–æ–≤ –º–æ–ª–∏—Ç–≤")
    
    for filename in files:
        filepath = os.path.join(prayers_dir, filename)
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–ª–æ–≤–∞ –∏–∑ –≤—Å–µ—Ö —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –ø–æ–ª–µ–π
            text_fields = ['content', 'contentModern', 'summary', 'explanation']
            for field in text_fields:
                if field in data and data[field]:
                    words = extract_words_from_text(data[field])
                    all_words.update(words)
                    print(f"  {filename}: {field} - {len(words)} —Å–ª–æ–≤")
                    
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞ {filename}: {e}")
    
    return all_words

def save_words_to_files(words: Set[str], words_per_file: int = 500):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å–ª–æ–≤–∞ –≤ —Ñ–∞–π–ª—ã –ø–æ —É–∫–∞–∑–∞–Ω–Ω–æ–º—É –∫–æ–ª–∏—á–µ—Å—Ç–≤—É"""
    words_list = sorted(list(words))
    total_words = len(words_list)
    
    print(f"–í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤: {total_words}")
    print(f"–ë—É–¥–µ—Ç —Å–æ–∑–¥–∞–Ω–æ —Ñ–∞–π–ª–æ–≤: {(total_words + words_per_file - 1) // words_per_file}")
    
    # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è —Ñ–∞–π–ª–æ–≤ —Å–æ —Å–ª–æ–≤–∞–º–∏
    words_dir = "extracted_words"
    os.makedirs(words_dir, exist_ok=True)
    
    # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Ñ–∞–π–ª—ã
    for i in range(0, total_words, words_per_file):
        chunk = words_list[i:i + words_per_file]
        filename = f"words_chunk_{i//words_per_file + 1:03d}.txt"
        filepath = os.path.join(words_dir, filename)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(f"# –°–ª–æ–≤–∞ {i+1}-{min(i+words_per_file, total_words)} –∏–∑ {total_words}\n")
            f.write(f"# –í—Å–µ–≥–æ —Å–ª–æ–≤ –≤ —Ñ–∞–π–ª–µ: {len(chunk)}\n\n")
            
            for word in chunk:
                f.write(f"{word}\n")
        
        print(f"–°–æ–∑–¥–∞–Ω —Ñ–∞–π–ª: {filepath} ({len(chunk)} —Å–ª–æ–≤)")

def create_word_frequency_file(words: Set[str]):
    """–°–æ–∑–¥–∞–µ—Ç —Ñ–∞–π–ª —Å —á–∞—Å—Ç–æ—Ç–æ–π —Å–ª–æ–≤"""
    prayers_dir = "data/prayers"
    word_counter = Counter()
    
    files = [f for f in os.listdir(prayers_dir) if f.endswith('.json')]
    
    for filename in files:
        filepath = os.path.join(prayers_dir, filename)
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # –°—á–∏—Ç–∞–µ–º —á–∞—Å—Ç–æ—Ç—É —Å–ª–æ–≤
            text_fields = ['content', 'contentModern', 'summary', 'explanation']
            for field in text_fields:
                if field in data and data[field]:
                    words_in_text = re.findall(r'\b[–∞-—è—ë]+\b', data[field].lower())
                    word_counter.update(words_in_text)
                    
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞ {filename}: {e}")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —á–∞—Å—Ç–æ—Ç—É —Å–ª–æ–≤
    words_dir = "extracted_words"
    filepath = os.path.join(words_dir, "word_frequency.txt")
    
    with open(filepath, 'w', encoding='utf-8') as f:
        f.write("# –ß–∞—Å—Ç–æ—Ç–∞ —Å–ª–æ–≤ –≤ –º–æ–ª–∏—Ç–≤–∞—Ö\n")
        f.write("# –§–æ—Ä–º–∞—Ç: —Å–ª–æ–≤–æ - –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤—Ö–æ–∂–¥–µ–Ω–∏–π\n\n")
        
        for word, count in word_counter.most_common():
            f.write(f"{word} - {count}\n")
    
    print(f"–°–æ–∑–¥–∞–Ω —Ñ–∞–π–ª —á–∞—Å—Ç–æ—Ç—ã —Å–ª–æ–≤: {filepath}")

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("üîç –ò–∑–≤–ª–µ–∫–∞–µ–º –≤—Å–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –º–æ–ª–∏—Ç–≤...")
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤—Å–µ —Å–ª–æ–≤–∞
    all_words = process_all_prayers()
    
    if not all_words:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Å–ª–æ–≤–∞")
        return
    
    print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
    print(f"  –í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤: {len(all_words)}")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–ª–æ–≤–∞ –≤ —Ñ–∞–π–ª—ã
    print(f"\nüíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–ª–æ–≤–∞ –≤ —Ñ–∞–π–ª—ã...")
    save_words_to_files(all_words, 500)
    
    # –°–æ–∑–¥–∞–µ–º —Ñ–∞–π–ª —Å —á–∞—Å—Ç–æ—Ç–æ–π —Å–ª–æ–≤
    print(f"\nüìà –°–æ–∑–¥–∞–µ–º —Ñ–∞–π–ª —Å —á–∞—Å—Ç–æ—Ç–æ–π —Å–ª–æ–≤...")
    create_word_frequency_file(all_words)
    
    print(f"\n‚úÖ –ì–æ—Ç–æ–≤–æ! –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø–∞–ø–∫—É 'extracted_words'")
    print(f"üìÅ –§–∞–π–ª—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞:")
    print(f"  - words_chunk_*.txt - —Å–ª–æ–≤–∞ –ø–æ 500 —à—Ç—É–∫")
    print(f"  - word_frequency.txt - —á–∞—Å—Ç–æ—Ç–∞ —Å–ª–æ–≤")

if __name__ == "__main__":
    main()
